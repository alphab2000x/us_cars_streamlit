{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import streamlit as st;\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "from sklearn.model_selection import cross_val_score;\n",
    "from sklearn.preprocessing import LabelEncoder;\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# pipeline elements\n",
    "from sklearn.decomposition import PCA # PCA = Principal Component Analysis\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# pipeline materiaux\n",
    "from sklearn.pipeline import Pipeline # PCA = Principal Component Analysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from stop_words import get_stop_words\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liens :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Télécharger le dataset : \n",
    "   <a href=\"https://drive.google.com/file/d/1OEWrVjE7B2d23-eQrTMcJpRJMxoB6iUH/view?usp=sharing \">labels.csv</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer une mesure de performance\n",
    "\n",
    "def accuracy(preds, target):\n",
    "    M = target.shape[0] # Nombre d'exemple\n",
    "    total_correctes = (preds == target).sum()\n",
    "    accuracy = total_correctes / M\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(list):\n",
    "    tmp = 0\n",
    "    proba = 0\n",
    "    for i in range(len(list)):\n",
    "        if list[i] > list[tmp]:\n",
    "            tmp = i\n",
    "            proba = list[i]\n",
    "    return tmp, proba\n",
    "\n",
    "def myClasse(classT, proba):\n",
    "    print(\" Classe : %d avec %f %%. \" % (classT, proba))\n",
    "    st.success(\" Classe : %d avec %f %%. \" % (classT, proba))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kn(tr, te):\n",
    "    st.success(\" L'accuracy est de : %f %% pour le train et de %f %% pour le test. \" % (tr*100, te*100))\n",
    "    imgTr = Image.open(\"img/knnTr.PNG\")\n",
    "    imgTe = Image.open(\"img/knnTe.PNG\")\n",
    "    st.image(imgTr, caption='Courbe de roc train',use_column_width=True)\n",
    "    st.image(imgTe, caption='Courbe de roc test',use_column_width=True)\n",
    "    return\n",
    "    \n",
    "def rf(tr, te):\n",
    "    st.success(\" L'accuracy est de : %f %% pour le train et de %f %% pour le test. \" % (tr*100, te*100))\n",
    "    imgTr = Image.open(\"img/rfTr.PNG\")\n",
    "    imgTe = Image.open(\"img/rfTe.PNG\")\n",
    "    st.image(imgTr, caption='Courbe de roc train',use_column_width=True)\n",
    "    st.image(imgTe, caption='Courbe de roc test',use_column_width=True)\n",
    "    return\n",
    "    \n",
    "def gb(tr, te):\n",
    "    st.success(\" L'accuracy est de : %f %% pour le train et de %f %% pour le test. \" % (tr*100, te*100))\n",
    "    imgTr = Image.open(\"img/gbTr.PNG\")\n",
    "    imgTe = Image.open(\"img/gbTe.PNG\")\n",
    "    st.image(imgTr, caption='Courbe de roc train',use_column_width=True)\n",
    "    st.image(imgTe, caption='Courbe de roc test',use_column_width=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accident(total, ville):\n",
    "    req1= \"(City == '\"\n",
    "    req2 = \"')\"\n",
    "    accident = total.query(req1+ville+req2).Accident\n",
    "    st.success(\" Dans la ville de %s, il y a eu %f accidents. \" % (ville, accident))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecte de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère notre Dataset et la stocke dans une variable\n",
    "\n",
    "accident = pd.read_csv('data/US_Accidents_June20_mini.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On supprime les colonnes qui nous intéressent pas\n",
    "\n",
    "accident = accident.drop(['ID'],axis=1)\n",
    "accident = accident.drop(['Start_Lat'],axis=1)\n",
    "accident = accident.drop(['Start_Lng'],axis=1)\n",
    "accident = accident.drop(['End_Lat'],axis=1)\n",
    "accident = accident.drop(['End_Lng'],axis=1)\n",
    "accident = accident.drop(['Description'],axis=1)\n",
    "accident = accident.drop(['Number'],axis=1)\n",
    "accident = accident.drop(['Street'],axis=1)\n",
    "accident = accident.drop(['Side'],axis=1)\n",
    "accident = accident.drop(['Zipcode'],axis=1)\n",
    "accident = accident.drop(['Country'],axis=1)\n",
    "accident = accident.drop(['Timezone'],axis=1)\n",
    "accident = accident.drop(['Weather_Timestamp'],axis=1)\n",
    "accident = accident.drop(['Wind_Direction'],axis=1)\n",
    "accident = accident.drop(['Wind_Chill(F)'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlève toutes les valeurs NaN\n",
    "\n",
    "accident = accident.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On conserve notre Dataset sans transformation\n",
    "\n",
    "accidentNoTransform = accident.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertie les colonnes dans le type qui nous interessent\n",
    "\n",
    "accident['Source'] = LabelEncoder().fit_transform(accident['Source'])\n",
    "\n",
    "accident.TMC = accident['TMC'].astype('category').cat.codes\n",
    "\n",
    "accident.City = accident['City'].astype('category').cat.codes\n",
    "\n",
    "accident.State = accident['State'].astype('category').cat.codes\n",
    "\n",
    "accident.County = accident['County'].astype('category').cat.codes\n",
    "\n",
    "accident.Airport_Code = accident['Airport_Code'].astype('category').cat.codes\n",
    "\n",
    "accident.Sunrise_Sunset =accident['Sunrise_Sunset'].astype('category').cat.codes \n",
    "\n",
    "accident.Weather_Condition = accident['Weather_Condition'].astype('category').cat.codes\n",
    "\n",
    "accident.Nautical_Twilight = accident['Nautical_Twilight'].astype('category').cat.codes\n",
    "\n",
    "accident.Astronomical_Twilight = accident['Astronomical_Twilight'].astype('category').cat.codes\n",
    "\n",
    "accident.Civil_Twilight = accident['Civil_Twilight'].astype('category').cat.codes\n",
    "\n",
    "accident['Amenity'] = LabelEncoder().fit_transform(accident['Amenity'])\n",
    "accident['Bump'] = LabelEncoder().fit_transform(accident['Bump'])\n",
    "accident['Crossing'] = LabelEncoder().fit_transform(accident['Crossing'])\n",
    "accident['Give_Way'] = LabelEncoder().fit_transform(accident['Give_Way'])\n",
    "accident['Junction'] = LabelEncoder().fit_transform(accident['Junction'])\n",
    "accident['No_Exit'] = LabelEncoder().fit_transform(accident['No_Exit'])\n",
    "accident['Railway'] = LabelEncoder().fit_transform(accident['Railway'])\n",
    "accident['Roundabout'] = LabelEncoder().fit_transform(accident['Roundabout'])\n",
    "accident['Station'] = LabelEncoder().fit_transform(accident['Station'])\n",
    "accident['Stop'] = LabelEncoder().fit_transform(accident['Stop'])\n",
    "accident['Traffic_Calming'] = LabelEncoder().fit_transform(accident['Traffic_Calming'])\n",
    "accident['Traffic_Signal'] = LabelEncoder().fit_transform(accident['Traffic_Signal'])\n",
    "accident['Turning_Loop'] = LabelEncoder().fit_transform(accident['Turning_Loop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va séparer notre target de nos colonnes\n",
    "\n",
    "Y = accident['Severity'].astype('category').cat.codes # La target va être la gravité\n",
    "X = accident.drop('Severity', axis='columns') # En fonction des critère environnant, on va essayer de prédir le niveau de gravité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On convertie nos column date de type object en type date\n",
    "\n",
    "X['Start_Time'] = pd.to_datetime(X['Start_Time'], \n",
    " format = '%Y-%m-%d %H:%M:%S', \n",
    " errors = 'coerce')\n",
    "X['End_Time'] = pd.to_datetime(X['End_Time'], \n",
    " format = '%Y-%m-%d %H:%M:%S', \n",
    " errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# On créé une colonne pour chaque élément de nos dates de début d'accident\n",
    "\n",
    "X['Start_Time_year'] = X['Start_Time'].dt.year\n",
    "X['Start_Time_month'] = X['Start_Time'].dt.month\n",
    "X['Start_Time_week'] = X['Start_Time'].dt.week\n",
    "X['Start_Time_day'] = X['Start_Time'].dt.day\n",
    "X['Start_Time_hour'] = X['Start_Time'].dt.hour\n",
    "X['Start_Time_minute'] = X['Start_Time'].dt.minute\n",
    "X['Start_Time_dayofweek'] = X['Start_Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Series.dt.weekofyear and Series.dt.week have been deprecated.  Please use Series.dt.isocalendar().week instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# On créé une colonne pour chaque élément de nos dates de fin d'accident\n",
    "\n",
    "X['End_Time_year'] = X['End_Time'].dt.year\n",
    "X['End_Time_month'] = X['End_Time'].dt.month\n",
    "X['End_Time_week'] = X['End_Time'].dt.week\n",
    "X['End_Time_day'] = X['End_Time'].dt.day\n",
    "X['End_Time_hour'] = X['End_Time'].dt.hour\n",
    "X['End_Time_minute'] = X['End_Time'].dt.minute\n",
    "X['End_Time_dayofweek'] = X['End_Time'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant que l'on a créé nos colonnes, on supprime nos de base vue que l'on en a plus besoin\n",
    "\n",
    "X = X.drop(['Start_Time'],axis=1)\n",
    "X = X.drop(['End_Time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enlève toutes les valeurs NaN\n",
    "\n",
    "X = X.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25482, 46), (25482,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = TTS(X, Y,              # features, target\n",
    "                            stratify = Y,       # Va prendre une proportion aux hasard de valeurs différentes histoire de ne pas avoir des cas où l'on a que des même valeur\n",
    "                            random_state=777,   # Sert à fixer le harsard pour ne pas avoir des résultat différents à chaque tests.\n",
    "                            train_size=0.8)     # 50% de X_train et Y_train et 50% de Y_test et Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(n_neighbors=21,\n",
    "         weights='uniform',\n",
    "         leaf_size=3)\n",
    "knn.fit(X_tr, Y_tr)\n",
    "train_preds = knn.predict(X_tr)\n",
    "predictions = knn.predict(X_te)\n",
    "knnTr = accuracy(train_preds, Y_tr)\n",
    "knnTe= accuracy(predictions, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0,      # Lecture aléatoire des données\n",
    "                            n_estimators=21,      # Il va diviser X_tr en plusieurs arbres\n",
    "                            max_depth=7)          # Il va spliter/augmenter la profondeur des arbres\n",
    "clf.fit(X_tr, Y_tr)\n",
    "train_preds = clf.predict(X_tr)\n",
    "predictions = clf.predict(X_te)\n",
    "clfTr = accuracy(train_preds, Y_tr)\n",
    "clfTe= accuracy(predictions, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = GradientBoostingClassifier(random_state=60,        # Lecture aléatoire des données\n",
    "                                 n_estimators=40,         # Nombre d'étape (modèle), plus il y a de modèle, plus il apprendra\n",
    "                                 max_depth=10)            # Profondeur de l'arborescence\n",
    "grad.fit(X_tr, Y_tr)\n",
    "train_preds = grad.predict(X_tr)\n",
    "predictions = grad.predict(X_te)\n",
    "gradTr = accuracy(train_preds, Y_tr)\n",
    "gradTe= accuracy(predictions, Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x281fca9f3c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title(\"US Car Accident\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x281fca9f3c8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.header(\"Algortihme de prédiction \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x281fca9f3c8>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.text(\"Nous voulons savoir quelle algorithme à la meilleurs accuracy:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = st.radio(\"Algorithme: \",(\"KNN\", \"RandomForest\",\"GradientBoosting\"))\n",
    "if status == 'KNN':\n",
    "    kn(knnTr, knnTe)\n",
    "if status == 'RandomForest':\n",
    "    rf(clfTr, clfTe)\n",
    "if status == 'GradientBoosting':\n",
    "    gb(gradTr, gradTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x281fca9f3c8>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.header(\"Nombre d'accident dans une ville\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x281fca9f3c8>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.text(\"Du faite qu'il y ai beaucoup de ville et que l'on peut pas afficher le nombre d'accident dans toutes les en 1 seul graphique ville, nous voulons permettre à l'utilisateur de pouvoir afficher le d'accident dans une ville précise:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdInput = st.text_input(\"Choisir une ville (ex: Chicago) : \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = accidentNoTransform\n",
    "df['Accident'] = df['City'] # On créé notre nouvelle colonne\n",
    "CityAccident = pd.DataFrame()            # On crée un nouveau dataframe\n",
    "CityAccident['City'] = df['City']        # On ajoute à notre dataset la colonne City\n",
    "CityAccident['Accident'] = 1             # On ajoute à notre dataset la valeur 1, cela nous permettra de faire la somme des city\n",
    "total = CityAccident.groupby(by=\"City\",as_index=False).sum().sort_values(by=\"Accident\", ascending=False) # On groupe tout nos country en en fonction de la somme de nos class par ordre décroissant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.button(\"Afficher\"):\n",
    "    Accident(total, stdInput)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
